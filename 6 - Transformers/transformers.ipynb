{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sh/1qclfzm54zlgld8yp8_l0bbh0000gn/T/ipykernel_84412/4157211178.py:14: DeprecationWarning: `set_matplotlib_formats` is deprecated since IPython 7.23, directly use `matplotlib_inline.backend_inline.set_matplotlib_formats()`\n",
      "  set_matplotlib_formats('svg', 'pdf') # For export\n"
     ]
    }
   ],
   "source": [
    "## Standard libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import json\n",
    "from functools import partial\n",
    "\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "plt.set_cmap('cividis')\n",
    "%matplotlib inline\n",
    "from IPython.display import set_matplotlib_formats\n",
    "set_matplotlib_formats('svg', 'pdf') # For export\n",
    "from matplotlib.colors import to_rgb\n",
    "import matplotlib\n",
    "matplotlib.rcParams['lines.linewidth'] = 2.0\n",
    "import seaborn as sns\n",
    "sns.reset_orig()\n",
    "\n",
    "## tqdm for loading bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "## PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "# Path to the folder where the datasets are/should be downloaded (e.g. CIFAR10)\n",
    "DATASET_PATH = \"../data\"\n",
    "# Path to the folder where the pretrained models are saved\n",
    "CHECKPOINT_PATH = \"../saved_models/tutorial6\"\n",
    "\n",
    "pl.seed_everything(42)\n",
    "\n",
    "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
    "torch.backends.cudnn.determinstic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(\"Device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/ReverseTask.ckpt...\n",
      "Downloading https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/SetAnomalyTask.ckpt...\n"
     ]
    }
   ],
   "source": [
    "# download two pre-trained models\n",
    "\n",
    "import urllib.request\n",
    "from urllib.error import HTTPError\n",
    "# Github URL where saved models are stored for this tutorial\n",
    "base_url = \"https://raw.githubusercontent.com/phlippe/saved_models/main/tutorial6/\"\n",
    "# Files to download\n",
    "pretrained_files = [\"ReverseTask.ckpt\", \"SetAnomalyTask.ckpt\"]\n",
    "\n",
    "# Create checkpoint path if it doesn't exist yet\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "# For each file, check whether it already exists. If not, try downloading it.\n",
    "for file_name in pretrained_files:\n",
    "    file_path = os.path.join(CHECKPOINT_PATH, file_name)\n",
    "    if \"/\" in file_name:\n",
    "        os.makedirs(file_path.rsplit(\"/\",1)[0], exist_ok=True)\n",
    "    if not os.path.isfile(file_path):\n",
    "        file_url = base_url + file_name\n",
    "        print(f\"Downloading {file_url}...\")\n",
    "        try:\n",
    "            urllib.request.urlretrieve(file_url, file_path)\n",
    "        except HTTPError as e:\n",
    "            print(\"Something went wrong. Please try to download the file from the GDrive folder, or contact the author with the full output including the following error:\\n\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Attention? \n",
    "The attention mechanism describes a recent new group of layers in neural networks that has attracted a lot of interest in the past few years, especially in sequence tasks. <br>\n",
    "\n",
    "*The attention mechanism describes a weighted average of (sequence) elements with the weights dynamically computed based on an input query and elements’ keys.* We want to dynamically decide on which inputs we want to “attend” more than others. <br>\n",
    "\n",
    "There are 4 usual parts: \n",
    "1. **Query**: feature vector that describes what we are looking for in the sequence i.e. what we maybe want to pay attention to\n",
    "2. **Keys**: for each input element, we have a key (a feature vector); roughly describes what this element is \"offering\" or when it might be important. Keys should be designed in a way in which we can identify the elements we want to pay attention to based on the query.\n",
    "3. **Values**: for each input element, we have a value vector. This is the feature vector that we want to average over.\n",
    "4. **Score Function**: to rate which elements we want to pay attention to, need to specify the scope function $f_{attn}$. Score function takes the query and the keys as input, and the output score/attention weight of the query-key pair. Usually implemented by simple metrics such as the dot product or a small MLP. <br>\n",
    "\n",
    "The weights of the avg are calculated by a softmax over all score function inputs. Hence, we assign those value vectors a higher weight whose corresponding key is most similar to the query.\n",
    "\n",
    "The attention applied inside the Transformer architecture is called **self-attention**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaled Dot Product Attention\n",
    "Allows for a network to attend over a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product(q, k, v, mask=None):\n",
    "    d_k = q.size()[-1] # hidden dimensionality of query and keys\n",
    "    attn_logits = torch.matmul(q, k.transpose(-2,-1)) # swaps the last and second to last dim\n",
    "    attn_logits = attn_logits / math.sqrt(d_k) # to scale dot product variance back down to original variance of Q and K\n",
    "    if mask is not None:\n",
    "        attn_logits = attn_logits.masked_fill(mask == 0, -9e15) # fill elements of attn_logits to -9e15 where mask == 0\n",
    "                                                                # shape of mask must be broadcastable with underlying tensor\n",
    "    attention = F.softmax(attn_logits, dim=-1)\n",
    "    values = torch.matmul(attention, v)\n",
    "    return values, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q\n",
      " tensor([[0.2666, 0.6274],\n",
      "        [0.2696, 0.4414],\n",
      "        [0.2969, 0.8317]])\n",
      "K\n",
      " tensor([[0.1053, 0.2695],\n",
      "        [0.3588, 0.1994],\n",
      "        [0.5472, 0.0062]])\n",
      "V\n",
      " tensor([[0.9516, 0.0753],\n",
      "        [0.8860, 0.5832],\n",
      "        [0.3376, 0.8090]])\n",
      "Values\n",
      " tensor([[0.7303, 0.4861],\n",
      "        [0.7262, 0.4902],\n",
      "        [0.7336, 0.4830]])\n",
      "Attention\n",
      " tensor([[0.3351, 0.3408, 0.3241],\n",
      "        [0.3302, 0.3390, 0.3308],\n",
      "        [0.3388, 0.3429, 0.3184]])\n"
     ]
    }
   ],
   "source": [
    "seq_len, d_k = 3, 2\n",
    "q = torch.rand(seq_len, d_k)\n",
    "k = torch.rand(seq_len, d_k)\n",
    "v = torch.rand(seq_len, d_k)\n",
    "values, attention = scaled_dot_product(q, k, v)\n",
    "print(\"Q\\n\", q)\n",
    "print(\"K\\n\", k)\n",
    "print(\"V\\n\", v)\n",
    "print(\"Values\\n\", values)\n",
    "print(\"Attention\\n\", attention) # will have dim (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention\n",
    "\n",
    "Often there are multiple different aspects a sequence element wants to attend to, and a single weighted average is not a good option for it.This is why we extend the attention mechanisms to multiple heads, i.e. multiple different query-key-value triplets on the same features. Given a query, key, and value matrix, we transform those into sub-queries, sub-keys, and sub-values, which we pass through the scaled dot product attention independently. Afterward, we concatenate the heads and combine them with a final weight matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be 0 modulo number of heads.\"\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # stack all weight matrices 1...h for efficiency\n",
    "        # NOTE: in many implementations, see \"bias=False\" which is optional\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3*embed_dim)\n",
    "        self.o_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        # original Transformer initialization\n",
    "        nn.init.xavier_uniform_(self.qkv_proj.weight)\n",
    "        self.qkv_proj.bias.data.fill_(0)\n",
    "        nn.init.xavier_uniform_(self.o_proj.weight)\n",
    "        self.o_proj.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, x, mask=None, return_attention=False):\n",
    "        batch_size, seq_len, embed_dim = x.size()\n",
    "        qkv = self.qkv_proj(x) # set current feature map in a neural network\n",
    "\n",
    "        # separate Q, K, V from linear ouput \n",
    "        qkv = qkv.reshape(batch_size, seq_len, self.num_heads, 3*self.head_dim)\n",
    "        qkv = qkv.permute(0, 2, 1, 3) # [batch, head, seq_len, dims]\n",
    "        q, k, v = qkv.chunk(3, dim=-1) # attempts to split a tensor into the specified number of chunks\n",
    "\n",
    "        # Determine value outputs\n",
    "        values, attention = scaled_dot_product(q, k, v, mask=mask)\n",
    "        values = values.permute(0, 2, 1, 3) # [batch, head, seq_len, dims]\n",
    "        values = values.reshape(batch_size, seq_len, embed_dim)\n",
    "        o = self.o_proj(values)\n",
    "\n",
    "        if return_attention:\n",
    "            return o, attention\n",
    "        else:\n",
    "            return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-head attention is looking at the input as a set, rather than a sequence (permutation-equivariant with respect to inputs). If it is important for the task to consider order, then it is common to encode the position in the input features. Compared to RNNs, self-attention layers can parallelize all of its operations making it much faster to compute for smaller sequence lengths. When the sequence length exceeds the hidden dimensionality, self-attention is more expensive to compute than an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder\n",
    "Originally, the Transformer model was designed for machine translation. Hence, it got an encoder-decoder structure where the encoder takes as input the sentence in the original language and generates an attention-based representation. On the other hand, the decoder attends over the encoded information and generates the translated sentence in an autoregressive manner, as in a standard RNN. <br>\n",
    "\n",
    "The encoder consists of $N$ identical blocks that are applied in sequence. The input $x$ is passed through a Multi-Head Attention layer. The output is then added to the original input using a residual connection and then a Layer Norm is applied. The residual connection is important for two reasons:\n",
    "1. Transformers are designed to be very deep. Residual connections are crucial for enabling smooth gradient flow through the model.\n",
    "2. W/o residual connection, info about the original sequence is lost. <br>\n",
    "\n",
    "Layer Norm enables faster training and provides small regularization. It also ensures that features are in a similar magnitude among the elements in a sequence. Batch Norm is NOT used becuase it depends on the batch size, which is normally very small for Transformers. <br>\n",
    "\n",
    "In addition to Multi-head Attention, a small fully connected feed-forward network is added to the model, which is applied to each position separately and identically. Can see this MLP as a post-process to prepare it for the next attention block. Usually, the dimensionality of this MLP is 2 to 8 times larger then $d_{model}$. General advantage of a wider layer instead of a narrow, multi-layer MLP is the faster, parallelizable execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, input_dim, num_heads, dim_feedforward, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_dim - Dimensionality of the input\n",
    "            num_heads - Number of heads to use in the attention block\n",
    "            dim_feedforward - Dimensionality of the hidden layer in the MLP\n",
    "            dropout - Dropout probability to use in the dropout layers\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Attentionlayer\n",
    "        self.self_attn = MultiheadAttention(input_dim, input_dim, num_heads)\n",
    "\n",
    "        # Two-Layer MLP\n",
    "        self.linear_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, dim_feedforward),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.ReLu(inplace=True),\n",
    "            nn.Linear(dim_feedforward, input_dim)\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b6358345afba150ae9d3d1df89aef232a9be0efc2622afd3e179bc22d42d3fe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl2021')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
